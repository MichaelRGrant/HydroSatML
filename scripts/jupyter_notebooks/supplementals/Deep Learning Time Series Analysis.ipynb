{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Time Series\n",
    "The hardest part is generally going to be shaping the data correctly. In doing so, you turn a time series problem into a general supervised learning problem. There are some caveats though. There has to be a constant series of data, meaning no breaks in time. Each row of the data needs to represent the same increment of time, i.e. day, hours, minutes etc.\n",
    "\n",
    "In order to accomplish this there are some great packages in R (and I'm sure Python as well) to do some time series imputation. Spline imputation is always a good choice to imputing missing data, but if you know the seasonality of the time series you can actually leverage more advanced techniques to do some very accurate imputation. The package I use most often is imputeTS which makes it really easy to visualize and impute missing data. There are also some tricks I use to make sure that there is a constant series in time. I will put together some R notebooks with some sample code/functions but once you get the idea of what needs to be done it should be fairly easy to do this in any language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import uniform\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Input, Dense, Conv1D, MaxPooling1D, AveragePooling1D, Dropout, Flatten, GRU, LSTM\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import *\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "import tensorflow as tf\n",
    "from pymlx import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_windowed(dataset, seq_length, horizon, feat_cols, resp_cols, region_col, split=0.2, resp_width=0,\n",
    "                        predict_current=False):\n",
    "    \"\"\"\n",
    "    This function takes data and creates a windowed dataframe to be used in time-series analysis. \n",
    "    dataset: [panda df] the raw dataframe that a time series windowed df will be created from\n",
    "    seq_length: [int] the sequence length, the amount of time to be used to perform the \n",
    "                time-series analysis\n",
    "    horizon: [int] how far into the future you wish to predict\n",
    "    feat_cols: [list] a list of column names that make up the feature space\n",
    "    resp_cols: [list] a list of column names that make up the response\n",
    "    region_col: [str] the name of the column that different time-series will be created on, i.e. \n",
    "                different regions that contain\n",
    "                independent time-series.\n",
    "    split: [float] A percent split for the test set, i.e. 0.2 equals a 80/20 split for the \n",
    "                train/test sets.\n",
    "    resp_width: [int] If you want to predict out to a set distance you set the horizon to that \n",
    "                    time point and this value to 0, however if you want to predict every value \n",
    "                    between let's say now and some point in the future you set horizon to 1 and\n",
    "                    the resp_width to that point. The algorithm will then predict every time point.\n",
    "    predict_current: [bool] horizon needs to be set to 1 for this to work. This will predict at \n",
    "                    the current time so if there is a sequence length of 2 instead of forecasting \n",
    "                    out the horizon length, the model\n",
    "                    will predict at the current time.\n",
    "    \"\"\"\n",
    "\n",
    "    if ((predict_current) and (horizon is not 1)):\n",
    "        raise ValueError('If predict_current is set to True, then Horizon must be set to 1.')\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if (any(dataset[feat_cols + resp_cols].isnull().sum()) != 0):\n",
    "        raise ValueError('There is missing data in at least one of the columns supplied in keep_cols. \\\n",
    "                         Please impute this missing data as needed.')\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # check to see if there are same features in both the response and the features list\n",
    "    resp_and_feats = [var for var in feat_cols if var in resp_cols]\n",
    "\n",
    "    regions = list(dataset[region_col].unique())\n",
    "    big_dict = {}\n",
    "\n",
    "    for i in range(len(regions)):\n",
    "        big_dict.update({regions[i]: dataset[dataset[region_col] == regions[i]]})\n",
    "    features = len(feat_cols)\n",
    "    response = len(resp_cols)\n",
    "    if resp_width == 0:\n",
    "        train_X_all, test_X_all = np.empty((0, seq_length, features)), np.empty((0, seq_length, features))\n",
    "        train_y_all, test_y_all = np.empty((0, response)), np.empty((0, response))\n",
    "    else:\n",
    "        train_X_all, test_X_all = np.empty((0, sequence_length, features)), np.empty((0, sequence_length, features))\n",
    "        train_y_all, test_y_all = np.empty((0, resp_width, response)), np.empty((0, resp_width, response))\n",
    "\n",
    "    for region in regions:\n",
    "        big_dict[region] = big_dict[region][resp_cols + feat_cols]\n",
    "        if resp_and_feats:\n",
    "            big_dict[region] = big_dict[region].loc[:, ~big_dict[region].columns.duplicated()]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        length = len(big_dict[region])\n",
    "        test_length = int(length * split)  # 20% test set\n",
    "\n",
    "        df_x = big_dict[region][feat_cols]\n",
    "        df_y = big_dict[region][resp_cols]\n",
    "        ts_x = df_x.values\n",
    "        ts_y = df_y.values\n",
    "\n",
    "        train_length = length - test_length\n",
    "\n",
    "        train_X, train_y, test_X, test_y = [], [], [], []\n",
    "\n",
    "        if (predict_current):\n",
    "            z = 2\n",
    "            q = 1\n",
    "        else:\n",
    "            z = 1\n",
    "            q = 0\n",
    "\n",
    "        if (resp_width != 0):\n",
    "            for i in range(train_length - sequence_length - horizon - resp_width):\n",
    "                train_X.append(ts_x[i:i + sequence_length])\n",
    "                train_y.append(ts_y[i + sequence_length + horizon - z:i + sequence_length + horizon - z + resp_width])\n",
    "            for i in range(-test_length, -resp_width):\n",
    "                test_X.append(ts_x[i - sequence_length - horizon + 1:i - horizon + 1])\n",
    "                test_y.append(ts_y[i - q:i - q + (resp_width)])\n",
    "\n",
    "        else:\n",
    "            for i in range(train_length - seq_length - horizon):\n",
    "                train_X.append(ts_x[i:i + seq_length])\n",
    "                train_y.append(ts_y[i + seq_length + horizon - z])\n",
    "            for i in range(-test_length, 0):\n",
    "                test_X.append(ts_x[i - seq_length - horizon + 1:i - horizon + 1])\n",
    "                test_y.append(ts_y[i - q])\n",
    "\n",
    "        train_X, train_y = np.array(train_X), np.array(train_y)\n",
    "        test_X, test_y = np.array(test_X), np.array(test_y)\n",
    "\n",
    "        train_X_all = np.append(train_X_all, train_X, axis=0)\n",
    "        test_X_all = np.append(test_X_all, test_X, axis=0)\n",
    "        train_y_all = np.append(train_y_all, train_y, axis=0)\n",
    "        test_y_all = np.append(test_y_all, test_y, axis=0)\n",
    "\n",
    "    # normalize\n",
    "    mean_x = train_X_all.mean(0)\n",
    "    std_x = train_X_all.std(0)\n",
    "\n",
    "    train_X = (train_X_all - mean_x) / std_x\n",
    "    test_X = (test_X_all - mean_x) / std_x\n",
    "    train_X = train_X.astype('float32')\n",
    "    test_X = test_X.astype('float32')\n",
    "\n",
    "    mean_y = train_y_all.mean(0)\n",
    "    std_y = train_y_all.std(0)\n",
    "\n",
    "    train_y = (train_y_all - mean_y) / std_y\n",
    "    test_y = (test_y_all - mean_y) / std_y\n",
    "    train_y = train_y.astype('float32')\n",
    "    test_y = test_y.astype('float32')\n",
    "\n",
    "    if resp_width != 0:\n",
    "        std_y = std_y.ravel()\n",
    "        mean_y = mean_y.ravel()\n",
    "        train_y = train_y.reshape(train_y.shape[0], train_y.shape[1] * response)\n",
    "        test_y = test_y.reshape(test_y.shape[0], test_y.shape[1] * response)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print('train_X shape:', np.shape(train_X))\n",
    "    print('train_y shape:', np.shape(train_y))\n",
    "    print('test_X shape:', np.shape(test_X))\n",
    "    print('test_y shape:', np.shape(test_y))\n",
    "\n",
    "    return train_X, test_X, train_y, test_y, mean_x, std_x, mean_y, std_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "\n",
    "def compute_mape_minMax(model, x, y):\n",
    "    forecasts = model.predict(x, batch_size=len(x))\n",
    "    forecasts = (forecasts *  (max_soilT - min_soilT)) + min_soilT\n",
    "    y_denom = (y * (max_soilT - min_soilT)) + min_soilT\n",
    "    return np.mean(np.abs((y_denom - forecasts) / y_denom))\n",
    "\n",
    "\n",
    "def compute_mape_centerScale(model, x, y):\n",
    "    forecasts = model.predict(x)\n",
    "#     forecasts = forecasts.reshape(forecasts.shape[0],dense)\n",
    "    forecasts = (forecasts * std_y) + mean_y\n",
    "    y_denom = (y * std_y) + mean_y\n",
    "    sub_result = np.abs((y_denom - forecasts) / y_denom )\n",
    "    # remove rows that are divided by 0 or a very small number\n",
    "    idxinf = np.where(sub_result == np.inf)\n",
    "    idx = np.where(sub_result > 1)\n",
    "    total_remove = len(idxinf[0]) + len(idx[0])\n",
    "    #print('Removed %f percent of the data, a total of %d rows.'% (((total_remove/len(forecasts))*100), total_remove))\n",
    "    sub_result = np.delete(sub_result, idxinf[0],0)\n",
    "    sub_result = np.delete(sub_result, idx[0], 0)\n",
    "    result = (np.mean(sub_result))\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_mae(model, x, y):\n",
    "    forecasts = model.predict(x)\n",
    "#     forecasts = forecasts.reshape(forecasts.shape[0],dense)\n",
    "    forecasts = (forecasts * std_y) + mean_y\n",
    "    y_denom = (y * std_y) + mean_y\n",
    "    result = mean_absolute_error(y_pred=forecasts, y_true=y_denom)\n",
    "    return result\n",
    "\n",
    "\n",
    "def test(model):\n",
    "    # print('Train Mean Absolute Percentage Error (MAPE): {0:.3f}'.format(compute_mape_centerScale(model, train_X, train_y)))\n",
    "    # print('Test Mean Absolute Percentage Error (MAPE): {0:.3f}'.format(compute_mape_centerScale(model, test_X, test_y)))\n",
    "    return(compute_mape_centerScale(model, train_X, train_y), compute_mape_centerScale(model, test_X, test_y))\n",
    "\n",
    "def test_mae(model):\n",
    "    yhat_train = model.predict(train_X)\n",
    "    yhat_test = model.predict(test_X)\n",
    "    return(compute_mae(model, train_X, train_y), compute_mae(model, test_X, test_y))\n",
    "\n",
    "\n",
    "def report(train, test):\n",
    "    train_mean = np.asarray(train).mean()\n",
    "    train_std = np.asarray(train).std()\n",
    "    test_mean = np.asarray(test).mean()\n",
    "    test_std = np.asarray(test).std()\n",
    "    train = str('Training MAE %f ± %f'% (train_mean, train_std))\n",
    "    test = str('Testing MAE %f ± %f'% (test_mean, test_std))\n",
    "    return(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>field</th>\n",
       "      <th>soilM_avg</th>\n",
       "      <th>imputed</th>\n",
       "      <th>precip.cm</th>\n",
       "      <th>tair.C</th>\n",
       "      <th>rh.pct</th>\n",
       "      <th>wind_sp.m_per_s</th>\n",
       "      <th>irradiance.w_per_m.2</th>\n",
       "      <th>average_sand</th>\n",
       "      <th>average_silt</th>\n",
       "      <th>average_clay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-10-29</td>\n",
       "      <td>AES</td>\n",
       "      <td>0.212173</td>\n",
       "      <td>0.212173</td>\n",
       "      <td>0.045</td>\n",
       "      <td>7.654167</td>\n",
       "      <td>68.083333</td>\n",
       "      <td>3.179167</td>\n",
       "      <td>121.708333</td>\n",
       "      <td>14.891866</td>\n",
       "      <td>60.796955</td>\n",
       "      <td>24.311179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-10-30</td>\n",
       "      <td>AES</td>\n",
       "      <td>0.210993</td>\n",
       "      <td>0.210993</td>\n",
       "      <td>0.369</td>\n",
       "      <td>9.354167</td>\n",
       "      <td>78.250000</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>14.891866</td>\n",
       "      <td>60.796955</td>\n",
       "      <td>24.311179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-10-31</td>\n",
       "      <td>AES</td>\n",
       "      <td>0.209983</td>\n",
       "      <td>0.209983</td>\n",
       "      <td>0.023</td>\n",
       "      <td>6.945833</td>\n",
       "      <td>68.625000</td>\n",
       "      <td>5.112500</td>\n",
       "      <td>122.666667</td>\n",
       "      <td>14.891866</td>\n",
       "      <td>60.796955</td>\n",
       "      <td>24.311179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-11-01</td>\n",
       "      <td>AES</td>\n",
       "      <td>0.209532</td>\n",
       "      <td>0.209532</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.225000</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>3.220833</td>\n",
       "      <td>127.500000</td>\n",
       "      <td>14.891866</td>\n",
       "      <td>60.796955</td>\n",
       "      <td>24.311179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-11-02</td>\n",
       "      <td>AES</td>\n",
       "      <td>0.207929</td>\n",
       "      <td>0.207929</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.862500</td>\n",
       "      <td>44.958333</td>\n",
       "      <td>6.091667</td>\n",
       "      <td>124.041667</td>\n",
       "      <td>14.891866</td>\n",
       "      <td>60.796955</td>\n",
       "      <td>24.311179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date field  soilM_avg   imputed  precip.cm    tair.C     rh.pct  \\\n",
       "0  2011-10-29   AES   0.212173  0.212173      0.045  7.654167  68.083333   \n",
       "1  2011-10-30   AES   0.210993  0.210993      0.369  9.354167  78.250000   \n",
       "2  2011-10-31   AES   0.209983  0.209983      0.023  6.945833  68.625000   \n",
       "3  2011-11-01   AES   0.209532  0.209532      0.000  2.225000  66.250000   \n",
       "4  2011-11-02   AES   0.207929  0.207929      0.000  4.862500  44.958333   \n",
       "\n",
       "   wind_sp.m_per_s  irradiance.w_per_m.2  average_sand  average_silt  \\\n",
       "0         3.179167            121.708333     14.891866     60.796955   \n",
       "1         3.562500             72.500000     14.891866     60.796955   \n",
       "2         5.112500            122.666667     14.891866     60.796955   \n",
       "3         3.220833            127.500000     14.891866     60.796955   \n",
       "4         6.091667            124.041667     14.891866     60.796955   \n",
       "\n",
       "   average_clay  \n",
       "0     24.311179  \n",
       "1     24.311179  \n",
       "2     24.311179  \n",
       "3     24.311179  \n",
       "4     24.311179  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CWD = os.getcwd()\n",
    "# FILE_PATH = os.path.join(CWD, \"../data/time_series_soilM_averaged_50percent_tuning.csv\")\n",
    "FILE_PATH = os.path.join(CWD, \"../data/time_series_soilM_averaged.csv\")\n",
    "dataset = pd.read_csv(FILE_PATH)\n",
    "dataset.head()\n",
    "dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "dataset = dataset.set_index(['date'])\n",
    "dataset = dataset.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_cols = ['imputed']\n",
    "feat_cols = ['precip.cm', 'tair.C', 'rh.pct', 'wind_sp.m_per_s', 'irradiance.w_per_m.2', 'average_clay',\n",
    "       'average_sand', 'average_silt']\n",
    "keep_cols = resp_cols + feat_cols\n",
    "\n",
    "sequence_length = 30\n",
    "horizon = 14\n",
    "resp_width = 0\n",
    "dense = len(resp_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Windowed DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X shape: (5244, 30, 8)\n",
      "train_y shape: (5244, 1)\n",
      "test_X shape: (1364, 30, 8)\n",
      "test_y shape: (1364, 1)\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y, mean_x, std_x, mean_y, std_y = make_windowed(dataset=dataset, \n",
    "                                                                                     seq_length=sequence_length,\n",
    "                                                                                     horizon=horizon,\n",
    "                                                                                     resp_cols=resp_cols,\n",
    "                                                                                     feat_cols=feat_cols,\n",
    "                                                                                     region_col='field',\n",
    "                                                                                     resp_width=resp_width,\n",
    "                                                                                     predict_current=False)\n",
    "dim = train_X.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Create a function that takes as input all the parameters for the deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(84)\n",
    "tf.set_random_seed(84)\n",
    "\n",
    "def create_tCNN_model(max_meanPool_1=1,\n",
    "                      max_meanPool_2=1,\n",
    "                      max_meanPool_3=1,\n",
    "                      max_meanPool_4=1,\n",
    "                      max_meanPool_5=1,\n",
    "                      max_meanPool_6=1,\n",
    "                      filters=32, \n",
    "                      kernel_size=1, \n",
    "                      pool_size=1, \n",
    "                      stride=1, \n",
    "                      optimizer='RMSprop', \n",
    "                      layers=1,\n",
    "                      init='glorot_normal',\n",
    "                      lr=0.001, \n",
    "                      momentum=0.9,\n",
    "                      decay=0.9,\n",
    "                      beta_1=0.9,\n",
    "                      beta_2=0.999,\n",
    "                      rho=0.9,\n",
    "                      nesterov=False,\n",
    "                      activation = ELU(),\n",
    "                      dropout_rate = 0.0,\n",
    "                      epsilon=1e-08,\n",
    "                      schedule_decay=0.004):\n",
    "    \n",
    "    sequence_input = Input(shape=(sequence_length, dim))\n",
    "            \n",
    "    if(layers == 1):\n",
    "        x = Conv1D(filters, kernel_size)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_1==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_1==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    elif(layers == 2):\n",
    "        x = Conv1D(filters, kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_1==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_1==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        x = Conv1D(int(filters/2), kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_2==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_2==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    elif(layers == 3):\n",
    "        x = Conv1D(filters, kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_1==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_1==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        x = Conv1D(int(filters/2), kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_2==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_2==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        x = Conv1D(int(filters/2/2), kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_3==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_3==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    elif(layers == 4):\n",
    "        x = Conv1D(filters, kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_1==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_1==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        x = Conv1D(int(filters/2), kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_2==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_2==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        x = Conv1D(int(filters/2/2), kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_3==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_3==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        x = Conv1D(int(filters/2/2/2), kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_4==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_4==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    elif(layers == 5):\n",
    "        x = Conv1D(filters, kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_1==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_1==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        x = Conv1D(int(filters/2), kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_2==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_2==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        x = Conv1D(int(filters/2/2), kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_3==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_3==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        x = Conv1D(int(filters/2/2/2), kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_4==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_4==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        x = Conv1D(int(filters/2/2/2/2), kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_5==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_5==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    else:\n",
    "        x = Conv1D(filters, kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_1==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_1==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        x = Conv1D(int(filters/2), kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_2==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_2==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        x = Conv1D(int(filters/2/2), kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_3==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_3==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        x = Conv1D(int(filters/2/2/2), kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_4==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_4==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        x = Conv1D(int(filters/2/2/2/2), kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_5==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_5==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        x = Conv1D(int(filters/2/2/2/2/2), kernel_size, init=init)(sequence_input)\n",
    "        x = activation(x)\n",
    "        if(max_meanPool_6==1):\n",
    "            x = MaxPooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        elif(max_meanPool_6==2):  \n",
    "            x = AveragePooling1D(pool_size=pool_size, stride=stride)(x)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(filters, init=init)(x)\n",
    "    x = activation(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    preds = Dense(dense)(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "\n",
    "    if(optimizer == 'SGD'):\n",
    "        sgd = SGD(lr=lr, momentum=momentum, decay=decay, nesterov=nesterov)\n",
    "    elif(optimizer == 'RMSprop'):\n",
    "        sgd = RMSprop(lr=lr, rho=rho)\n",
    "    elif(optimizer == 'Nadam'):\n",
    "        sgd = Nadam(lr=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, schedule_decay=schedule_decay)\n",
    "    elif(optimizer == 'Adagrad'):\n",
    "        sgd = Adagrad(lr=lr, epsilon=epsilon, decay=decay)\n",
    "    elif(optimizer == 'Adadelta'):\n",
    "        sgd = Adadelta(lr=lr, rho=rho, epsilon=epsilon, decay=decay)\n",
    "    elif(optimizer == 'Adam'):\n",
    "        sgd = Adam(lr=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, decay=decay)\n",
    "    elif(optimizer == 'Adamax'):\n",
    "        sgd = Adamax(lr=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, decay=decay)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    model.compile(loss='mae', optimizer=sgd, metrics=['mse'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the parameter tuning space that you will tune over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecasting out 14 days, with a 30.0 day window. \n",
      "Feature space:  ['precip.cm', 'tair.C', 'rh.pct', 'wind_sp.m_per_s', 'irradiance.w_per_m.2', 'average_clay', 'average_sand', 'average_silt']\n",
      "Response:  ['imputed']\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Epoch 1/5\n",
      "3496/3496 [==============================] - 1s 167us/step - loss: 0.6306 - mean_squared_error: 0.7320\n",
      "Epoch 2/5\n",
      "3496/3496 [==============================] - 0s 45us/step - loss: 0.5370 - mean_squared_error: 0.5742\n",
      "Epoch 3/5\n",
      "3496/3496 [==============================] - 0s 41us/step - loss: 0.5130 - mean_squared_error: 0.5316\n",
      "Epoch 4/5\n",
      "3496/3496 [==============================] - 0s 42us/step - loss: 0.5001 - mean_squared_error: 0.5110\n",
      "Epoch 5/5\n",
      "3496/3496 [==============================] - 0s 45us/step - loss: 0.4918 - mean_squared_error: 0.4979\n",
      "1748/1748 [==============================] - 0s 50us/step\n",
      "3496/3496 [==============================] - 0s 21us/step\n",
      "Epoch 1/5\n",
      "3496/3496 [==============================] - 1s 162us/step - loss: 0.6482 - mean_squared_error: 0.7460\n",
      "Epoch 2/5\n",
      "3496/3496 [==============================] - 0s 44us/step - loss: 0.5630 - mean_squared_error: 0.5863\n",
      "Epoch 3/5\n",
      "3496/3496 [==============================] - 0s 41us/step - loss: 0.5470 - mean_squared_error: 0.5506\n",
      "Epoch 4/5\n",
      "3496/3496 [==============================] - 0s 39us/step - loss: 0.5383 - mean_squared_error: 0.5359\n",
      "Epoch 5/5\n",
      "3496/3496 [==============================] - 0s 43us/step - loss: 0.5313 - mean_squared_error: 0.5267\n",
      "1748/1748 [==============================] - 0s 61us/step\n",
      "3496/3496 [==============================] - 0s 17us/step\n",
      "Epoch 1/5\n",
      "3496/3496 [==============================] - 1s 170us/step - loss: 0.6273 - mean_squared_error: 0.6430\n",
      "Epoch 2/5\n",
      "3496/3496 [==============================] - 0s 47us/step - loss: 0.4730 - mean_squared_error: 0.3781\n",
      "Epoch 3/5\n",
      "3496/3496 [==============================] - 0s 50us/step - loss: 0.4469 - mean_squared_error: 0.3346\n",
      "Epoch 4/5\n",
      "3496/3496 [==============================] - 0s 52us/step - loss: 0.4358 - mean_squared_error: 0.3180\n",
      "Epoch 5/5\n",
      "3496/3496 [==============================] - 0s 50us/step - loss: 0.4289 - mean_squared_error: 0.3102\n",
      "1748/1748 [==============================] - 0s 68us/step\n",
      "3496/3496 [==============================] - 0s 18us/step\n",
      "Elapsed: 0.098 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    5.9s finished\n"
     ]
    }
   ],
   "source": [
    "print('Forecasting out {0} days, with a {1:0.1f} day window. '.format(horizon, (sequence_length)))\n",
    "print('Feature space: ', feat_cols)\n",
    "print('Response: ', resp_cols)\n",
    "\n",
    "model = KerasRegressor(build_fn=create_tCNN_model, shuffle=True, verbose=1)\n",
    "\n",
    "# init = ['uniform', 'normal', 'zero', 'glorot_uniform', 'glorot_normal', 'lecun_uniform', \n",
    "#                       'lecun_normal', 'he_normal', 'he_uniform']\n",
    "# optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "\n",
    "layers = [1,2,3,4,5,6]\n",
    "\n",
    "nesterov = [False]\n",
    "filters = [32,64,128,256,512] # 32 is the lowest number of filters for tuning up to 6 layers\n",
    "batch_size = [2,4,8,16,32,64]\n",
    "epochs = [5]\n",
    "lr = [10**k for k in range(-5, -1)]\n",
    "max_meanPool_1=[1,2]\n",
    "max_meanPool_2=[1,2]\n",
    "max_meanPool_3=[1,2]\n",
    "max_meanPool_4=[1,2]\n",
    "max_meanPool_5=[1,2]\n",
    "max_meanPool_6=[1,2]\n",
    "kernel_size=[1,2,3,4]\n",
    "pool_size=[1,2,3,4]\n",
    "stride=[1,2,3,4]\n",
    "\n",
    "optimizer = ['Adam', 'Nadam', 'Adamax', 'RMSprop']\n",
    "init = ['glorot_uniform', 'glorot_normal', 'lecun_uniform', 'lecun_normal']\n",
    "momentum=[0.9]\n",
    "dropout_rate=[0.0]\n",
    "decay = np.arange(0,.1,0.001).tolist()\n",
    "# decay = np.arange(0.06,0.1,0.001).tolist()\n",
    "beta_1 = np.random.uniform(low=0.90, high=1.0, size=20).tolist()\n",
    "beta_2 = np.random.uniform(0.90, 1.0, 20).tolist()\n",
    "rho = np.random.uniform(0.80, 0.95, 20).tolist()\n",
    "epsilon = [1e-09, 1e-08,1e-07, 1e-06, 1e-05, 1e-04]\n",
    "schedule_decay = np.arange(0.001,0.01,0.0005).tolist()\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, \n",
    "                  epsilon=epsilon,\n",
    "                  schedule_decay=schedule_decay,\n",
    "                  epochs=epochs, \n",
    "                  layers=layers, \n",
    "                  filters=filters,\n",
    "                  optimizer=optimizer,\n",
    "                  lr=lr,\n",
    "                  decay=decay,\n",
    "                  init=init,\n",
    "                  beta_1=beta_1,\n",
    "                  beta_2=beta_2,\n",
    "                  nesterov=nesterov,\n",
    "                  rho=rho,\n",
    "                  pool_size=pool_size,\n",
    "                  kernel_size=kernel_size,\n",
    "                  stride=stride,\n",
    "                  momentum=momentum,\n",
    "                  dropout_rate=dropout_rate,\n",
    "                  max_meanPool_1=max_meanPool_1, \n",
    "                  max_meanPool_2=max_meanPool_2, \n",
    "                  max_meanPool_3=max_meanPool_3, \n",
    "                  max_meanPool_4=max_meanPool_4, \n",
    "                  max_meanPool_5=max_meanPool_5,\n",
    "                  max_meanPool_6=max_meanPool_6)\n",
    "\n",
    "start_timing()\n",
    "sweeper = random_sweep(\n",
    "    train_X, train_y, \n",
    "    model, param_grid,\n",
    "    scoring=compute_mae, \n",
    "    n_iter=1, n_jobs=1, \n",
    "    refit=False, cv=3, verbose=1)\n",
    "report_timing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sweep_stats(sweeper)\n",
    "FILE_PATH = os.path.join(CWD, \"../results/best_params/forecasting/tCNN_hourlyData-25percent_SEEDFEATS-DP-RH-WindGust-Speed_Resp-ForeCast-LW-8-2-VWC_H1_Rw120_W360_Tune_run1.csv\")\n",
    "results.to_csv(FILE_PATH)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM/GRU\n",
    "## Same approach as above just with a different model. If you want to run a GRU, just change LSTM to GRU on line 23. GRU's generally run faster than LSTM but essentially the same backend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Forecasting out {0} days, with a {1} day window. '.format(horizon, sequence_length))\n",
    "print('Feature space: ', feat_cols)\n",
    "print('Response: ', resp_cols)\n",
    "np.random.seed(84)\n",
    "tf.set_random_seed(84)\n",
    "\n",
    "def create_LSTM_model(layers=1, \n",
    "                 optimizer='adam', \n",
    "                 units=5, \n",
    "                 lr=0.001, \n",
    "                 momentum=0.9, \n",
    "                 decay=0.9,\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 beta_1=0.9,\n",
    "                 beta_2=0.999,\n",
    "                 rho=0.9):\n",
    "    \n",
    "    np.random.seed(84)\n",
    "    tf.set_random_seed(84)\n",
    "    \n",
    "    model = Sequential()\n",
    "    for i in range(layers):\n",
    "        model.add(LSTM(units, input_shape=(sequence_length, dim_x),\n",
    "                       return_sequences=True if i < layers-1 else False))\n",
    "        \n",
    "    model.add(Dense(dim_y, kernel_initializer=kernel_initializer))\n",
    "    \n",
    "    if(optimizer == 'SGD'):\n",
    "        sgd = SGD(lr=lr, momentum=momentum, decay=decay, nesterov=False)\n",
    "    elif(optimizer == 'RMSprop'):\n",
    "        sgd = RMSprop(lr=lr, rho=rho)\n",
    "    elif(optimizer == 'Nadam'):\n",
    "        sgd = Nadam(lr=lr, beta_1=beta_1, beta_2=beta_2, epsilon=1e-08, schedule_decay=0.004)\n",
    "    elif(optimizer == 'Adagrad'):\n",
    "        sgd = Adagrad(lr=lr, epsilon=1e-08, decay=decay)\n",
    "    elif(optimizer == 'Adadelta'):\n",
    "        sgd = Adadelta(lr=lr, rho=0.95, epsilon=1e-08, decay=decay)\n",
    "    elif(optimizer == 'Adam'):\n",
    "        sgd = Adam(lr=lr, beta_1=beta_1, beta_2=beta_2, epsilon=1e-08, decay=decay)\n",
    "    elif(optimizer == 'Adamax'):\n",
    "        sgd = Adamax(lr=lr, beta_1=beta_1, beta_2=beta_2, epsilon=1e-08, decay=decay)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    model.compile(loss='mae', optimizer=sgd, metrics=['mse'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasRegressor(build_fn=create_model, shuffle=True, verbose=0)\n",
    "\n",
    "# kernel_initializer = ['uniform', 'normal', 'zero', 'glorot_uniform', 'glorot_normal', 'lecun_uniform', \n",
    "#                       'lecun_normal', 'he_normal', 'he_uniform']\n",
    "# optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "\n",
    "layers = [1]\n",
    "units = [2,4,8,16,32,64,128,256,512]\n",
    "batch_size = [2,4,8,16,32]\n",
    "epochs = [50]\n",
    "# lr = [0.0005, 0.001, 0.005, 0.01, 0.05]\n",
    "lr = [10**k for k in range(-5, -1)]\n",
    "beta_1 = np.random.uniform(low=0.9, high=1.0, size=20).tolist()\n",
    "# beta_1 = [0.9]\n",
    "beta_2 = np.random.uniform(0.95, 1.0, 20).tolist()\n",
    "# beta_2 = [0.999]\n",
    "decay = np.arange(0,.2,0.01).tolist()\n",
    "rho = np.random.uniform(0.8, 1.0, 20).tolist()\n",
    "# decay = [0, 0.05, 0.01, 0.015, 0.02, 0.025]\n",
    "# decay = np.concatenate([np.arange(0,.1,0.01).tolist(), np.arange(0.95,1,0.01).tolist()]).tolist()\n",
    "\n",
    "kernel_initializer = ['normal', 'zero', 'glorot_uniform', 'glorot_normal', 'lecun_uniform', \n",
    "                       'lecun_normal', 'he_normal', 'he_uniform']\n",
    "optimizer = ['RMSprop', 'Adam', 'Adamax', 'Nadam']\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, \n",
    "                  epochs=epochs, \n",
    "                  layers=layers, \n",
    "                  units=units,\n",
    "                  optimizer=optimizer,\n",
    "                  lr=lr,\n",
    "                  decay=decay,\n",
    "                  kernel_initializer=kernel_initializer,\n",
    "                  beta_1=beta_1,\n",
    "                  beta_2=beta_2)\n",
    "\n",
    "start_timing()\n",
    "sweeper = random_sweep(\n",
    "    train_X, train_y, \n",
    "    model, param_grid,\n",
    "    scoring=compute_mae, \n",
    "    n_iter=15, n_jobs=6, refit=False, cv=tscv, verbose=1)\n",
    "report_timing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
